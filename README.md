# ğŸ¸ Goumis - Fine-tuning GPT-2 com Greentexts do 4chan

Este projeto realiza o fine-tuning do modelo GPT-2 utilizando um dataset de greentexts coletados do 4chan. 

## ğŸ“– Sobre o Projeto

O **Goumis** Ã© um experimento de aprendizado de mÃ¡quina que treina o modelo de linguagem GPT-2 da OpenAI para gerar textos no estilo caracterÃ­stico das "greentexts" â€” histÃ³rias curtas e humorÃ­sticas originadas nos fÃ³runs do 4chan, tipicamente escritas em linhas que comeÃ§am com `>`.

## ğŸ¯ Objetivo

O objetivo principal Ã© fazer com que o modelo aprenda o estilo Ãºnico de escrita das greentexts, incluindo:
- Formato de texto com linhas iniciando com `>`
- Narrativa em primeira pessoa
- Tom humorÃ­stico e absurdo
- Estrutura tÃ­pica de "histÃ³ria de anÃ´nimo"

## ğŸ“¦ DependÃªncias

O projeto utiliza as seguintes bibliotecas principais:

- **transformers** (>=4.57.3) - Para carregar e treinar o modelo GPT-2
- **torch** (>=2. 9.1) - Framework de deep learning
- **datasets** (>=4. 4.1) - Para manipulaÃ§Ã£o do dataset
- **tiktoken** (>=0.12.0) - TokenizaÃ§Ã£o
- **tqdm** (>=4.67.1) - Barras de progresso

## ğŸš€ InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone https://github. com/mnsgrosa/goumis.git
cd goumis

# Instale as dependÃªncias usando uv
uv sync

# Ou usando pip
pip install -e .
```

## ğŸ“‚ Estrutura do Projeto

```
goumis/
â”œâ”€â”€ main.py              # Script principal
â”œâ”€â”€ src/                 # CÃ³digo fonte do projeto
â”œâ”€â”€ greentext_data/      # Dataset de greentexts
â”œâ”€â”€ log/                 # Logs de treinamento
â”œâ”€â”€ pyproject.toml       # ConfiguraÃ§Ãµes do projeto
â””â”€â”€ README.md            # Este arquivo
```

## ğŸ—ƒï¸ Dataset

O dataset utilizado consiste em greentexts coletadas do 4chan.  Greentexts sÃ£o um formato de postagem caracterÃ­stico dos imageboards, onde as linhas comeÃ§am com o sÃ­mbolo `>` (que aparece em verde no site original, daÃ­ o nome). 

### CaracterÃ­sticas do Dataset:
- Formato de texto Ãºnico e reconhecÃ­vel
- HistÃ³rias curtas e narrativas
- ConteÃºdo humorÃ­stico e satÃ­rico
- Linguagem informal da internet

## ğŸ§  Sobre o GPT-2

O GPT-2 (Generative Pre-trained Transformer 2) Ã© um modelo de linguagem desenvolvido pela OpenAI. AtravÃ©s do processo de fine-tuning, adaptamos o modelo prÃ©-treinado para gerar textos especÃ­ficos no estilo greentext.

### Processo de Treinamento:
1. Carregamento do modelo GPT-2 prÃ©-treinado
2. PreparaÃ§Ã£o e tokenizaÃ§Ã£o do dataset de greentexts
3. Fine-tuning do modelo com os dados especÃ­ficos
4.  AvaliaÃ§Ã£o e geraÃ§Ã£o de novos textos

## ğŸ“ Uso

```python
from src import main

# Execute o treinamento
python main.py
```

## âš ï¸ Aviso

Este projeto Ã© puramente educacional e experimental. O conteÃºdo gerado pelo modelo pode refletir o estilo e tom do dataset de treinamento.  Use com responsabilidade. 

## ğŸ“„ LicenÃ§a

Este projeto Ã© de cÃ³digo aberto.  Sinta-se livre para usar, modificar e distribuir. 

## ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas!  Sinta-se Ã  vontade para abrir issues ou pull requests.

---

*Feito com ğŸ¸ e muito fine-tuning*
